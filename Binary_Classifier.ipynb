{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary Classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1htFsqFpN1-"
      },
      "source": [
        "**Binary classification Model on Large Movie Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2orHPbtQHuH",
        "outputId": "526e20b0-f377-4df0-ab5c-5b46015488c3"
      },
      "source": [
        "from tensorflow import keras\n",
        "from pathlib import Path\n",
        "\n",
        "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
        "FILENAME = \"aclImdb_v1.tar.gz\"\n",
        "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\n",
        "path = Path(filepath).parent / \"aclImdb\"\n",
        "path"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 2s 0us/step\n",
            "84140032/84125825 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.keras/datasets/aclImdb')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VOOuWOOQKET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09b08d33-bd4c-4d84-f48a-6b885cd1da25"
      },
      "source": [
        "import os\n",
        "from tensorflow import keras\n",
        "from pathlib import Path\n",
        "\n",
        "for name, subdirs, files in os.walk(path):\n",
        "    indent = len(Path(name).parts) - len(path.parts)\n",
        "    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n",
        "    for index, filename in enumerate(sorted(files)):\n",
        "        if index == 3:\n",
        "            print(\"    \" * (indent + 1) + \"...\")\n",
        "            break\n",
        "        print(\"    \" * (indent + 1) + filename)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aclImdb/\n",
            "    README\n",
            "    imdb.vocab\n",
            "    imdbEr.txt\n",
            "    train/\n",
            "        labeledBow.feat\n",
            "        unsupBow.feat\n",
            "        urls_neg.txt\n",
            "        ...\n",
            "        neg/\n",
            "            0_3.txt\n",
            "            10000_4.txt\n",
            "            10001_4.txt\n",
            "            ...\n",
            "        pos/\n",
            "            0_9.txt\n",
            "            10000_8.txt\n",
            "            10001_10.txt\n",
            "            ...\n",
            "        unsup/\n",
            "            0_0.txt\n",
            "            10000_0.txt\n",
            "            10001_0.txt\n",
            "            ...\n",
            "    test/\n",
            "        labeledBow.feat\n",
            "        urls_neg.txt\n",
            "        urls_pos.txt\n",
            "        neg/\n",
            "            0_2.txt\n",
            "            10000_4.txt\n",
            "            10001_1.txt\n",
            "            ...\n",
            "        pos/\n",
            "            0_10.txt\n",
            "            10000_7.txt\n",
            "            10001_9.txt\n",
            "            ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEwqM5mRoWmm",
        "outputId": "20ce2013-dc7a-409b-a26d-ad2466d76722"
      },
      "source": [
        "def review_paths(dirpath):\n",
        "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
        "\n",
        "train_pos = review_paths(path / \"train\" / \"pos\")\n",
        "train_neg = review_paths(path / \"train\" / \"neg\")\n",
        "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
        "test_valid_neg = review_paths(path / \"test\" / \"neg\")\n",
        "\n",
        "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12500, 12500, 12500, 12500)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdkTRMyhpF_d"
      },
      "source": [
        "**Splitting the test set into a validation set (15,000) and a test set (10,000).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_OnsdTQo3j0"
      },
      "source": [
        "import numpy as np\n",
        "np.random.shuffle(test_valid_pos)\n",
        "\n",
        "test_pos = test_valid_pos[:5000]\n",
        "test_neg = test_valid_neg[:5000]\n",
        "valid_pos = test_valid_pos[5000:]\n",
        "valid_neg = test_valid_neg[5000:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lRDzUQepBGf"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "def imdb_dataset(filepaths_positive, filepaths_negative):\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
        "        for filepath in filepaths:\n",
        "            with open(filepath) as review_file:\n",
        "                reviews.append(review_file.read())\n",
        "            labels.append(label)\n",
        "    return tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.constant(reviews), tf.constant(labels)))\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDdjmwztqM6a",
        "outputId": "9cb0adeb-3f8f-431c-dd53-a46adffcf458"
      },
      "source": [
        "for X, y in imdb_dataset(train_pos, train_neg).take(3):\n",
        "    print(X)\n",
        "    print(y)\n",
        "    print()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b\"You've got to admire director Todd Sheets for his dedication, drive and enthusiasm when it comes to movie-making: between 1985 and 2000, he made a whopping 34 films. Unfortunately, if his Zombie Bloodbath trilogy is anything to go by, they're probably all crap (and a quick look at their IMDb ratings seems to verify my hunch).<br /><br />Part 3 sees a group of obnoxious students finding detention a little more eventful than usual after they are attacked by hordes of the living dead, who have escaped from a top-secret army base located directly beneath their school. Working from a dreadful script by Brian Eklund (which relies heavily on liberal use of the f-bomb) director Sheets delivers yet another embarrassingly amateurish effort featuring some mind-numbingly awful performances from his talent-free cast, dreadful visual effects (some crap CGI and what looks like the front of a giant cardboard space-shuttle) and his trademark shoddy gore (handfuls of offal pulled from beneath his victims' clothing).<br /><br />Finally, after what seems like an eternity watching irritating characters running for their lives, and unconvincing undead people fondling animal innards, Zombie Armageddon finishes with a time-travel/paradox twist ending which forces viewers to re-watch several torturous minutes from the beginning of the film. Honestly... once was enough, Mr. Sheets\\xc2\\x97what have we done to deserve having to watch it again?\", shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "\n",
            "tf.Tensor(b'Looked forward to viewing this film and seeing these great actors perform. However, I was sadly disappointed in the script and the entire plot of the story. David Duchovny,(Dr. Eugene Sands),\"Connie & Carla\",\\'04, was the doctor in the story who uses drugs and losses his license to practice medicine. Dr. Sands was visiting a night club and was able to use his medical experience to help a wounded customer and was assisted by Angelina Jolie,(Claire),\"Taking Lives\",\\'04, who immediately becomes attracted to Dr. David Sands. Timothy Hutton,(Raymond Blossom),\"Kinsey\",\\'04, plays the Big Shot Gangster and a man with all kinds of money and connections. Timothy Hutton seems to over act in most of the scenes and goes completely out of his mind trying to keep his gang members from being killed. Gary Dourdan,(Yates),\"CSI-Vegas TV Series\", plays a great supporting role and portrays a real COOL DUDE who is a so-called body guard for Raymond Blossom. Angelina Jolie looks beautiful and sexy with her ruby red lips which draws a great deal of attention from all the men. This film is not the greatest, but it does entertain.', shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "\n",
            "tf.Tensor(b'It is so rare that I get to rate a movie without having some reservation as to whether I should have gone up one or down one but this one.....Did the explosion rate a notch higher, or one down because my brain hurt trying to CREATE a plot. No, THIS ONE....yeah, a solid, no brainer.....ONE/ten', shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFWbDsSUqPU7",
        "outputId": "ede3d3b7-0426-4014-9318-b08193aa0685"
      },
      "source": [
        "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).cache().repeat(10): pass"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 1: 41.5 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVLXVSRAquuJ"
      },
      "source": [
        "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
        "    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n",
        "                                          num_parallel_reads=n_read_threads)\n",
        "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
        "    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n",
        "                                          num_parallel_reads=n_read_threads)\n",
        "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
        "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIQdhmWorudF",
        "outputId": "461b9b6f-e108-4da7-fa37-4b0c746e7356"
      },
      "source": [
        "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).cache().repeat(10): pass"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 1: 29.8 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSZBMtDdr5OS"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
        "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
        "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAdM5wEAsXuW",
        "outputId": "e1652863-e551-452e-80c0-2f36ef25dd5b"
      },
      "source": [
        "def preprocess(X_batch, n_words=50):\n",
        "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
        "    Z = tf.strings.substr(X_batch, 0, 300)\n",
        "    Z = tf.strings.lower(Z)\n",
        "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
        "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
        "    Z = tf.strings.split(Z)\n",
        "    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n",
        "\n",
        "X_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\n",
        "preprocess(X_example)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 50), dtype=string, numpy=\n",
              "array([[b'it', b's', b'a', b'great', b'great', b'movie', b'i', b'loved',\n",
              "        b'it', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n",
              "       [b'it', b'was', b'terrible', b'run', b'away', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
              "        b'<pad>']], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz6DJbh4e_UE"
      },
      "source": [
        "**Created a Custom Pre-Processor for cleaning . **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBcd3A4SskKA",
        "outputId": "36bd184e-eac9-4b2b-f1c1-8336ec452ccc"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_vocabulary(data_sample, max_size=1000):\n",
        "    preprocessed_reviews = preprocess(data_sample).numpy()\n",
        "    counter = Counter()\n",
        "    for words in preprocessed_reviews:\n",
        "        for word in words:\n",
        "            if word != b\"<pad>\":\n",
        "                counter[word] += 1\n",
        "    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]\n",
        "\n",
        "get_vocabulary(X_example)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'<pad>',\n",
              " b'it',\n",
              " b'great',\n",
              " b's',\n",
              " b'a',\n",
              " b'movie',\n",
              " b'i',\n",
              " b'loved',\n",
              " b'was',\n",
              " b'terrible',\n",
              " b'run',\n",
              " b'away']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMZs2230uN1_"
      },
      "source": [
        "class TextVectorization(keras.layers.Layer):\n",
        "    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        self.max_vocabulary_size = max_vocabulary_size\n",
        "        self.n_oov_buckets = n_oov_buckets\n",
        "\n",
        "    def adapt(self, data_sample):\n",
        "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n",
        "        words = tf.constant(self.vocab)\n",
        "        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n",
        "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        preprocessed_inputs = preprocess(inputs)\n",
        "        return self.table.lookup(preprocessed_inputs)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOQ5yYx8uRH8",
        "outputId": "93b5b90d-28ae-47f4-82ee-d4fa780b3643"
      },
      "source": [
        "text_vectorization = TextVectorization()\n",
        "\n",
        "text_vectorization.adapt(X_example)\n",
        "text_vectorization(X_example)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
              "array([[ 1,  3,  4,  2,  2,  5,  6,  7,  1,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0],\n",
              "       [ 1,  8,  9, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwEQYbVJuS8C"
      },
      "source": [
        "max_vocabulary_size = 1000\n",
        "n_oov_buckets = 100\n",
        "\n",
        "sample_review_batches = train_set.map(lambda review, label: review)\n",
        "sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),\n",
        "                                axis=0)\n",
        "\n",
        "text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,\n",
        "                                       input_shape=[])\n",
        "text_vectorization.adapt(sample_reviews)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgshzjQruZak",
        "outputId": "18028b95-d00e-4398-cea6-c0dd488461db"
      },
      "source": [
        "text_vectorization(X_example)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
              "array([[  9,  14,   2,  64,  64,  12,   5, 257,   9,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  9,  13, 269, 530, 335,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1FNgTN0uddE",
        "outputId": "78231024-dbcb-489f-e65b-a6c132c3500f"
      },
      "source": [
        "\n",
        "text_vectorization.vocab[:10]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'<pad>', b'the', b'a', b'of', b'and', b'i', b'to', b'is', b'this', b'it']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QphyxVUHufwE",
        "outputId": "07c6baa1-2a87-4b83-f74e-167be3263875"
      },
      "source": [
        "simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\n",
        "tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
              "array([[2., 2., 0., 1.],\n",
              "       [3., 0., 2., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy6RoDo1uif6"
      },
      "source": [
        "class BagOfWords(keras.layers.Layer):\n",
        "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        self.n_tokens = n_tokens\n",
        "    def call(self, inputs):\n",
        "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
        "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIMGHozauuX4",
        "outputId": "4b2c38c0-714f-4ceb-dcaf-49eb98e53954"
      },
      "source": [
        "bag_of_words = BagOfWords(n_tokens=4)\n",
        "bag_of_words(simple_example)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
              "array([[2., 0., 1.],\n",
              "       [0., 2., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnUbJbWpuxBX"
      },
      "source": [
        "n_tokens = max_vocabulary_size + n_oov_buckets + 1 # add 1 for <pad>\n",
        "bag_of_words = BagOfWords(n_tokens)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd1-eZrxuzQE",
        "outputId": "07fca6c1-3e4f-4dac-8ac9-e8688420895b"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    text_vectorization,\n",
        "    bag_of_words,\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_set, epochs=5, validation_data=valid_set)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 20s 17ms/step - loss: 0.5429 - accuracy: 0.7192 - val_loss: 0.5104 - val_accuracy: 0.7421\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 16s 16ms/step - loss: 0.4663 - accuracy: 0.7770 - val_loss: 0.5159 - val_accuracy: 0.7349\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 16s 16ms/step - loss: 0.4100 - accuracy: 0.8138 - val_loss: 0.5142 - val_accuracy: 0.7453\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 16s 17ms/step - loss: 0.3328 - accuracy: 0.8618 - val_loss: 0.5373 - val_accuracy: 0.7379\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 16s 17ms/step - loss: 0.2429 - accuracy: 0.9139 - val_loss: 0.5788 - val_accuracy: 0.7338\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff296c600d0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gqnz2cCehcN"
      },
      "source": [
        "**The Binary Classifier reaches a validation Accuracy of 73%**. "
      ]
    }
  ]
}